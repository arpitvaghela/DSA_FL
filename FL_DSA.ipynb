{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXKyzM4rCEWaviyO3NxE/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitvaghela/DSA_FL/blob/main/FL_DSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svCGSaj-nkhH"
      },
      "source": [
        "# LSTM time series prediction using Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXCLSy4Gnwlb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH133PxAmIwg"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFY4S2_Snf7n"
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMWb0_6TiCEY",
        "outputId": "57423453-1e33-4ec9-c1a9-f81f601e0b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow_federated as tff\n",
        "\n",
        "# Test the TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20201029). \n",
            "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
            "If you encounter a bug, do not file an issue on GitHub.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sai_FSFon2bi"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01sVWKEeGD2h",
        "outputId": "9dba51e5-4457-4f78-cbee-28c5a5c5efea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "FM = np.fromfile('96_7_20db',dtype=np.float32)\n",
        "FM.reshape(-1,1) # data0 will follow a distribution when generated "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08798511],\n",
              "       [0.16339657],\n",
              "       [0.28623393],\n",
              "       ...,\n",
              "       [0.14975475],\n",
              "       [0.1355331 ],\n",
              "       [0.02713103]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAM2qAD-oW94"
      },
      "source": [
        "### Bandpower Equation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYO80pFIoeQA"
      },
      "source": [
        "def bandpower(signal:np.array)->np.float:\n",
        "  return np.mean(signal ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNXbJyA7oi1m",
        "outputId": "9cd2c607-a403-4b31-a440-dc4354818ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bandpower(FM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08850595"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcmk3Fto4Rv"
      },
      "source": [
        "### awgn Function\n",
        "\n",
        "Function to add noise to the signal resulting in given S/N ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWbyqmZGo8_W"
      },
      "source": [
        "def awgn(signal:np.ndarray, desired_snr:int):\n",
        "  \"\"\"Add AWGN noise to generate signal with given SNR. \n",
        "  \"\"\"\n",
        "  # Converting the SNR from dB scale to linear scale\n",
        "  snr_linear = math.pow(10, desired_snr / 10)\n",
        "  \n",
        "  # Standard normally distributed noise\n",
        "  noise = np.random.randn(signal.shape[0], 1)\n",
        "  \n",
        "  # Using the boxed formula\n",
        "  var_signal = bandpower(noise) * snr_linear\n",
        "  \n",
        "  # Normalizing the signal to have the given variance\n",
        "  normalized_signal = math.sqrt(var_signal) * (signal / math.sqrt(bandpower(signal)))\n",
        "  \n",
        "  #print(\"SNR = \" + str(10 * math.log10(bandpower(normalized_signal) / bandpower(noise))))\n",
        "  \n",
        "  return normalized_signal + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4pR1LeimNOH"
      },
      "source": [
        "## Filtering Data\n",
        "\n",
        "filtering data points to be in range $10^{-7}< signal< 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIk4gvwImMLw",
        "outputId": "0b86f6e1-1e9a-4c4e-feb7-3f01d8ef7ff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "FM = FM[np.logical_and(FM > math.pow(10, -7), FM < 1)]\n",
        "FM = FM.reshape(FM.shape[0], 1)\n",
        "print(\"Size of FM: \" + str(FM.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of FM: (2971649, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbLdLGRQmiRO"
      },
      "source": [
        "## Creating Dataset\n",
        "\n",
        "- take datapoints of size, _samples*sample_size_\n",
        "\n",
        "- add noise with desire snr\n",
        "\n",
        "- sample = $[s_1,s_2,\\cdots,s_N]$\n",
        "\n",
        "- Energy detection = $\\sum_{i=1}^{N}s_i^2$\n",
        "\n",
        "- $X[j]$ = $\\sum_{i=j*N+1}^{(j+1)*N}s_i^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LYkOJNmk0T"
      },
      "source": [
        "def create_dataset(signal, desired_snr, samples, sample_size):\n",
        "  \n",
        "  # Creating the signal with desired SNR\n",
        "  snr_signal = awgn(signal[0:samples * sample_size], desired_snr)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = snr_signal[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WulnZ62AqrIo",
        "outputId": "a48e982b-30c2-4005-94b8-8e907e3e3901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# sample dataset for SNR=4\n",
        "\n",
        "print(create_dataset(FM[50000:], 4, 15000, 100).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNR = 4.000000453350213\n",
            "(15000, 1)\n",
            "CPU times: user 294 ms, sys: 2.78 ms, total: 297 ms\n",
            "Wall time: 298 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_gucdavtx1G"
      },
      "source": [
        "def final_dataset(signal, snr_range, samples_per_snr, sample_size):\n",
        "  X = {}\n",
        "  \n",
        "  for snr in snr_range:\n",
        "    # Creating dataset for the given SNR\n",
        "    X_snr = create_dataset(signal, snr, samples_per_snr, sample_size)\n",
        "    \n",
        "    # Indexing within the final dataset matrix X\n",
        "    X[snr] = X_snr\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4xprvuz63F"
      },
      "source": [
        "## Generating White Noise Sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxedtzrhz1Zg"
      },
      "source": [
        "def create_noise_sequence(samples, sample_size):\n",
        "  \n",
        "  # Creating white noise sequence of variance 1\n",
        "  noise = np.random.randn(samples * sample_size, 1)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = noise[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJyk2JF-75mK"
      },
      "source": [
        "## DataSet LookBack for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvufTiZ474jN"
      },
      "source": [
        "# Function for Chaning the dataset for look back  #linear dataset initially\n",
        "def create_look_back(X, look_back=1):\n",
        "  \n",
        "  # Look back dataset is initialized to be empty\n",
        "  look_back_X = []\n",
        "  \n",
        "  for i in range(len(X) - look_back + 1):\n",
        "    # Extracting an example from the dataset\n",
        "    a = X[i:(i + look_back), :]\n",
        "    \n",
        "    a = a.flatten() # (For flattening) #1D list\n",
        "    \n",
        "    # Appending to the dataset\n",
        "    look_back_X.append(a)\n",
        "  \n",
        "#  look_back_Y = []\n",
        "    \n",
        "  # Returning in numpy's array format\n",
        "  return np.array(look_back_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GMlbPO8AVW"
      },
      "source": [
        "def dataset_look_back(X_tech, snr_range, look_back):\n",
        "  X_tech_lb = {}\n",
        "  \n",
        "  # Look backs for all SNRs\n",
        "  for snr in snr_range:\n",
        "    X_tech_lb[snr] = create_look_back(X_tech[snr], look_back)\n",
        "  \n",
        "  return X_tech_lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scC_nkQK5nTW"
      },
      "source": [
        "def generate_Dataset(FM,snr_ratio=(0.8, 0.2),sample_size=100,total_sample=100000,lsnr_range=(-20,-4),hsnr_range=(-4,6),look_back = 2,eval=False):\n",
        "\n",
        "  no_of_sample_hsnr = int((total_sample*snr_ratio[0])//((hsnr_range[1] - hsnr_range[0])/2))\n",
        "  no_of_sample_lsnr = int((total_sample*snr_ratio[1])//((lsnr_range[1] - lsnr_range[0])/2))\n",
        "  \n",
        "  X_FM = {**final_dataset(FM[100000:], range(lsnr_range[0],lsnr_range[1], 2),no_of_sample_lsnr, sample_size),\n",
        "          **final_dataset(FM[100000:], range(hsnr_range[0],hsnr_range[1], 2), no_of_sample_hsnr, sample_size)}\n",
        "\n",
        "  X_noise = create_noise_sequence(100000, 100)\n",
        "\n",
        "  # lookback\n",
        "\n",
        "  X_FM_lb = dataset_look_back(X_FM, range(-20, 6, 2), look_back)\n",
        "  X_noise_lb = create_look_back(X_noise, look_back)\n",
        "\n",
        "  # final X_train and y\n",
        "\n",
        "  X = X_FM_lb[-20]\n",
        "  y = []\n",
        "\n",
        "  for snr in range(-18, 6, 2):\n",
        "    X = np.concatenate((X, X_FM_lb[snr]), axis=0)\n",
        "\n",
        "  y = np.ones((X.shape[0], 1))\n",
        " \n",
        "  # print(X.shape)\n",
        "  # print(X_noise_lb.shape)\n",
        "  X = np.concatenate((X, X_noise_lb), axis=0)\n",
        "  y_train = np.concatenate((y, np.zeros((X_noise_lb.shape[0], 1))))\n",
        "\n",
        "  # reshape\n",
        "  X_train = np.reshape(X, (-1, 2, 1))\n",
        "  Y_train = np.reshape(y_train,(-1,1,1))\n",
        "  return X_train,Y_train\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJU9EU3wBwNe"
      },
      "source": [
        "# created dataset\n",
        "X,Y = generate_Dataset(FM[100000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGzoklAWn1D2",
        "outputId": "817d68ca-3736-433c-c83f-2b8b4d5db1f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Shape of X,Y :\",X.shape,Y.shape) # similar to the shape we have\n",
        "\n",
        "# train,test split\n",
        "train_size = int(len(X)*0.80)\n",
        "test_size = len(X) - train_size\n",
        "\n",
        "X_train, X_test = X[:train_size,:,:],X[train_size:,:,:]\n",
        "Y_train, Y_test = Y[:train_size,:,:],Y[train_size:,:,:]\n",
        "\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(Y_train.shape,Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X,Y : (199986, 2, 1) (199986, 1, 1)\n",
            "(159988, 2, 1) (39998, 2, 1)\n",
            "(159988, 1, 1) (39998, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toI_ufHp468o",
        "outputId": "80e955a6-b1b0-4aba-ac2f-51d3545b872b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train[0],Y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[113.70631322],\n",
              "        [ 91.70375081]]), array([[1.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgDaE52i1rW1"
      },
      "source": [
        "## LSTM model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFVvvCA1oyA",
        "outputId": "8e5d2aee-09c7-4eb4-96fa-9f8f67c381a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 4)                 96        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 101\n",
            "Trainable params: 101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcsCPMCG2TiW",
        "outputId": "d183f7d9-71cf-463f-e092-86e0fcabec6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(X_train,Y_train,validation_split=0.25,epochs=10,verbose=1)\n",
        "\n",
        "model.evaluate(X_test,Y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3750/3750 [==============================] - 11s 2ms/step - loss: 0.1772 - accuracy: 0.8308 - val_loss: 0.2869 - val_accuracy: 0.3795\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0974 - accuracy: 0.8507 - val_loss: 0.4082 - val_accuracy: 0.1701\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0900 - accuracy: 0.8558 - val_loss: 0.2807 - val_accuracy: 0.5006\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0879 - accuracy: 0.8568 - val_loss: 0.2656 - val_accuracy: 0.5539\n",
            "Epoch 5/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0869 - accuracy: 0.8586 - val_loss: 0.2712 - val_accuracy: 0.5465\n",
            "Epoch 6/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0871 - accuracy: 0.8576 - val_loss: 0.3786 - val_accuracy: 0.3041\n",
            "Epoch 7/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0865 - accuracy: 0.8583 - val_loss: 0.2096 - val_accuracy: 0.7326\n",
            "Epoch 8/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0868 - accuracy: 0.8577 - val_loss: 0.3870 - val_accuracy: 0.2970\n",
            "Epoch 9/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0864 - accuracy: 0.8575 - val_loss: 0.2221 - val_accuracy: 0.7042\n",
            "Epoch 10/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0870 - accuracy: 0.8562 - val_loss: 0.2364 - val_accuracy: 0.6651\n",
            "1250/1250 [==============================] - 1s 1ms/step - loss: 0.2333 - accuracy: 0.6740\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2333240956068039, 0.6740337014198303]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRyulTQ4xlR",
        "outputId": "88f099f4-4ab9-4b9e-bd75-3d6d099d9104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.predict([\n",
        "                [[0.88069264],[0.60346049]]\n",
        "               ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8407477]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyrzeQJ5Lr8"
      },
      "source": [
        "## Federated Training and preprocessing centrailized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmtnpe0k7LkP"
      },
      "source": [
        "### Generating a decentralized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-mdaAoCIZd9",
        "outputId": "74cebc7d-88b2-4f40-d5ae-4f82a0810d27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "T_CLIENTS = 20\n",
        "CLIENT_SAMPLE_SIZE = 2000\n",
        "step = len(X)/T_CLIENTS\n",
        "\n",
        "data_fed = [ tf.data.Dataset.from_tensor_slices(\n",
        "     \n",
        "         {\"value\":X[int(i*step):int((i+1)*step)],\"label\":Y[int(i*step):int((i+1)*step)]}\n",
        "      \n",
        "     \n",
        "     ) for i in range(T_CLIENTS)]\n",
        "\n",
        "# client dataset can be accesed as data_fed[ CLIENT_ID ]\n",
        "example_dataset = data_fed[0]\n",
        "\n",
        "# def preprocess(data_fed):\n",
        "#   def batch_format_fn(ele):\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "\n",
        "print(example_element[\"value\"].numpy()) # example_element[0] refers to X val\n",
        "print(example_element[\"label\"].numpy()) # example_element[1] refers to X val\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[113.70631322]\n",
            " [ 91.70375081]]\n",
            "[[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Wpqli6Vh2a"
      },
      "source": [
        "# preprocess\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "  \n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=element[\"value\"],\n",
        "        y=element[\"label\"]\n",
        "    )\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMHiDzm2WtjI",
        "outputId": "abe5d1ed-9ea7-4bcf-914f-793b55300d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch[\"x\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMom1cGNXsRR"
      },
      "source": [
        "def make_federated_data(client_data,client_ids):\n",
        "  return [\n",
        "      preprocess(client_data[x])\n",
        "      for x in client_ids\n",
        "  ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgHIDe4YB1w",
        "outputId": "39b323f2-70b5-4a93-d87e-6cea8a227829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_clients = [x for x in range(NUM_CLIENTS)]\n",
        "\n",
        "federated_train_data = make_federated_data(data_fed, sample_clients)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of client datasets: 10\n",
            "First dataset: <PrefetchDataset shapes: OrderedDict([(x, (None, 2, 1)), (y, (None, 1, 1))]), types: OrderedDict([(x, tf.float64), (y, tf.float64)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KncNTVBU5JXO",
        "outputId": "911d7e9c-c4e3-444d-ee4b-e21f363ca0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "    keras_model,\n",
        "    input_spec=preprocessed_example_dataset.element_spec,\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.BinaryCrossentropy()]\n",
        "  )\n",
        "\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "str(iterative_process.initialize.type_signature)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'( -> <model=<trainable=<float32[1,16],float32[4,16],float32[16],float32[4,1],float32[1]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybQ_rTkjRfm"
      },
      "source": [
        "state = iterative_process.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Z1XlOgjgqK",
        "outputId": "f8d60d18-b63c-424c-e2fb-07cb875443c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "NUM_ROUNDS = 10\n",
        "for round_num in range(0, NUM_ROUNDS):\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num+1, metrics))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.17952451), ('loss', 0.033268273)]))])\n",
            "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.069182105), ('loss', 0.004552643)]))])\n",
            "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.051077876), ('loss', 0.0025265298)]))])\n",
            "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.042241532), ('loss', 0.001754196)]))])\n",
            "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.036775947), ('loss', 0.0013472522)]))])\n",
            "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.032977022), ('loss', 0.001096452)]))])\n",
            "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.030143708), ('loss', 0.00092662236)]))])\n",
            "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.027927928), ('loss', 0.0008041225)]))])\n",
            "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.026134828), ('loss', 0.00071165175)]))])\n",
            "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.024645874), ('loss', 0.00063941395)]))])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}