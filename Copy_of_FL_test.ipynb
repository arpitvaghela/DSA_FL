{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FL_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitvaghela/DSA_FL/blob/main/Copy_of_FL_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svCGSaj-nkhH"
      },
      "source": [
        "# LSTM time series prediction using Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXCLSy4Gnwlb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH133PxAmIwg"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFY4S2_Snf7n"
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMWb0_6TiCEY",
        "outputId": "2035425d-c9ff-456a-9e81-b1ae6ba0f1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "import tensorflow_federated as tff\n",
        "\n",
        "# Test the TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6e88d3a11f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_federated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Test the TFF is working:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Hello, World!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_federated/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maggregators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/aggregators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclipping_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZeroingClippingFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclipping_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZeroingFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDifferentiallyPrivateFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAggregationProcessFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeanFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/aggregators/dp_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_privacy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_privacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_privacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy_ledger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianSumQueryEntry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_privacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy_ledger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrivacyLedger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_privacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy_ledger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryWithLedger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_privacy/privacy/analysis/privacy_ledger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_privacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprivacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muser_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/types/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TensorLike'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sai_FSFon2bi"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01sVWKEeGD2h"
      },
      "source": [
        "#FM = np.fromfile('96_7_20db',dtype=np.float32)\n",
        "N = 10**6\n",
        "FM = np.random.randn(N) # normal distributed channel\n",
        "FM.reshape(-1,1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAM2qAD-oW94"
      },
      "source": [
        "### Bandpower Equation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYO80pFIoeQA"
      },
      "source": [
        "def bandpower(signal:np.array)->np.float:\n",
        "  return np.mean(signal ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNXbJyA7oi1m"
      },
      "source": [
        "bandpower(FM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcmk3Fto4Rv"
      },
      "source": [
        "### awgn Function\n",
        "\n",
        "Function to add noise to the signal resulting in given S/N ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWbyqmZGo8_W"
      },
      "source": [
        "def awgn(signal:np.ndarray, desired_snr:int):\n",
        "  \"\"\"Add AWGN noise to generate signal with given SNR. \n",
        "  \"\"\"\n",
        "  # Converting the SNR from dB scale to linear scale\n",
        "  snr_linear = math.pow(10, desired_snr / 10)\n",
        "  \n",
        "  # Standard normally distributed noise\n",
        "  noise = np.random.randn(signal.shape[0], 1)\n",
        "  \n",
        "  # Using the boxed formula\n",
        "  var_signal = bandpower(noise) * snr_linear\n",
        "  \n",
        "  # Normalizing the signal to have the given variance\n",
        "  normalized_signal = math.sqrt(var_signal) * (signal / math.sqrt(bandpower(signal)))\n",
        "  \n",
        "  #print(\"SNR = \" + str(10 * math.log10(bandpower(normalized_signal) / bandpower(noise))))\n",
        "  \n",
        "  return normalized_signal + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4pR1LeimNOH"
      },
      "source": [
        "## Filtering Data\n",
        "\n",
        "filtering data points to be in range $10^{-7}< signal< 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIk4gvwImMLw"
      },
      "source": [
        "FM = FM[np.logical_and(FM > math.pow(10, -7), FM < 1)]\n",
        "FM = FM.reshape(FM.shape[0], 1)\n",
        "print(\"Size of FM: \" + str(FM.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbLdLGRQmiRO"
      },
      "source": [
        "## Creating Dataset\n",
        "\n",
        "- take datapoints of size, _samples*sample_size_\n",
        "\n",
        "- add noise with desire snr\n",
        "\n",
        "- sample = $[s_1,s_2,\\cdots,s_N]$\n",
        "\n",
        "- Energy detection = $\\sum_{i=1}^{N}s_i^2$\n",
        "\n",
        "- $X[j]$ = $\\sum_{i=j*N+1}^{(j+1)*N}s_i^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LYkOJNmk0T"
      },
      "source": [
        "def create_dataset(signal, desired_snr, samples, sample_size):\n",
        "  \n",
        "  # Creating the signal with desired SNR\n",
        "  snr_signal = awgn(signal[0:samples * sample_size], desired_snr)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = snr_signal[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WulnZ62AqrIo"
      },
      "source": [
        "%%time\n",
        "# sample dataset for SNR=4\n",
        "\n",
        "print(create_dataset(FM[50000:], 4, 15000, 100).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_gucdavtx1G"
      },
      "source": [
        "def final_dataset(signal, snr_range, samples_per_snr, sample_size):\n",
        "  X = {}\n",
        "  \n",
        "  for snr in snr_range:\n",
        "    # Creating dataset for the given SNR\n",
        "    X_snr = create_dataset(signal, snr, samples_per_snr, sample_size)\n",
        "    \n",
        "    # Indexing within the final dataset matrix X\n",
        "    X[snr] = X_snr\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4xprvuz63F"
      },
      "source": [
        "## Generating White Noise Sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxedtzrhz1Zg"
      },
      "source": [
        "def create_noise_sequence(samples, sample_size):\n",
        "  \n",
        "  # Creating white noise sequence of variance 1\n",
        "  noise = np.random.randn(samples * sample_size, 1)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = noise[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJyk2JF-75mK"
      },
      "source": [
        "## DataSet LookBack for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvufTiZ474jN"
      },
      "source": [
        "# Function for Chaning the dataset for look back  #linear dataset initially\n",
        "def create_look_back(X, look_back=1):\n",
        "  \n",
        "  # Look back dataset is initialized to be empty\n",
        "  look_back_X = []\n",
        "  \n",
        "  for i in range(len(X) - look_back + 1):\n",
        "    # Extracting an example from the dataset\n",
        "    a = X[i:(i + look_back), :]\n",
        "    \n",
        "    a = a.flatten() # (For flattening) #1D list\n",
        "    \n",
        "    # Appending to the dataset\n",
        "    look_back_X.append(a)\n",
        "  \n",
        "#  look_back_Y = []\n",
        "    \n",
        "  # Returning in numpy's array format\n",
        "  return np.array(look_back_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GMlbPO8AVW"
      },
      "source": [
        "def dataset_look_back(X_tech, snr_range, look_back):\n",
        "  X_tech_lb = {}\n",
        "  \n",
        "  # Look backs for all SNRs\n",
        "  for snr in snr_range:\n",
        "    X_tech_lb[snr] = create_look_back(X_tech[snr], look_back)\n",
        "  \n",
        "  return X_tech_lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scC_nkQK5nTW"
      },
      "source": [
        "def generate_Dataset(FM,snr_ratio=(0.8, 0.2),sample_size=100):\n",
        "  lsnr_range=(-20,-4)\n",
        "  hsnr_range=(-4,6)\n",
        "  look_back = 2\n",
        "  N = len(FM)\n",
        "  \n",
        "  no_of_sample_hsnr = int((N*snr_ratio[0])//((hsnr_range[1] - hsnr_range[0])/2))\n",
        "  no_of_sample_lsnr = int((N*snr_ratio[1])//((lsnr_range[1] - lsnr_range[0])/2))\n",
        "  \n",
        "  X_FM = {**final_dataset(FM, range(lsnr_range[0],lsnr_range[1], 2),no_of_sample_lsnr, sample_size),\n",
        "          **final_dataset(FM, range(hsnr_range[0],hsnr_range[1], 2), no_of_sample_hsnr, sample_size)}\n",
        "\n",
        "  X_noise = create_noise_sequence(N, sample_size)\n",
        "\n",
        "  # lookback\n",
        "\n",
        "  X_FM_lb = dataset_look_back(X_FM, range(-20, 6, 2), look_back)\n",
        "  X_noise_lb = create_look_back(X_noise, look_back)\n",
        "\n",
        "  # final X_train and y\n",
        "\n",
        "  X = X_FM_lb[-20]\n",
        "  y = []\n",
        "\n",
        "  for snr in range(-18, 6, 2):\n",
        "    X = np.concatenate((X, X_FM_lb[snr]), axis=0)\n",
        "\n",
        "  y = np.ones((X.shape[0], 1))\n",
        " \n",
        "  # print(X.shape)\n",
        "  # print(X_noise_lb.shape)\n",
        "  X = np.concatenate((X, X_noise_lb), axis=0)\n",
        "  y_train = np.concatenate((y, np.zeros((X_noise_lb.shape[0], 1))))\n",
        "\n",
        "  \n",
        "  # reshape\n",
        "  X_train = np.reshape(X, (-1, 2, 1))\n",
        "  Y_train = np.reshape(y_train,(-1,1,1))\n",
        "\n",
        "  # index = [i for i in range(len(X_train))]\n",
        "  # np.random.shuffle(index)\n",
        "\n",
        "  # X_train = np.array([X_train[i] for i in index])\n",
        "  # Y_train = np.array([Y_train[i] for i in index])\n",
        "  \n",
        "  return X_train,Y_train\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJU9EU3wBwNe"
      },
      "source": [
        "# created dataset\n",
        "X,Y = generate_Dataset(FM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGzoklAWn1D2"
      },
      "source": [
        "print(\"Shape of X,Y :\",X.shape,Y.shape) # similar to the shape we have\n",
        "\n",
        "# train,test split\n",
        "train_size = int(len(X)*0.80)\n",
        "test_size = len(X) - train_size\n",
        "\n",
        "X_train, X_test = X[:train_size,:,:],X[train_size:,:,:]\n",
        "Y_train, Y_test = Y[:train_size,:,:],Y[train_size:,:,:]\n",
        "\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(Y_train.shape,Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toI_ufHp468o"
      },
      "source": [
        "X_train[0],Y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgDaE52i1rW1"
      },
      "source": [
        "## LSTM model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFVvvCA1oyA"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcsCPMCG2TiW"
      },
      "source": [
        "model.fit(X_train,Y_train,validation_split=0.25,epochs=3,verbose=1)\n",
        "\n",
        "model.evaluate(X_test,Y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmsKdN4L-XoH"
      },
      "source": [
        "## LSTM model test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRyulTQ4xlR"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "Y_pred = model.predict(X_test).ravel()\n",
        "Y_pred.reshape(-1,1)\n",
        "fpr, tpr, threshold = roc_curve(Y_test.reshape(-1,1), Y_pred) \n",
        "auc_tf = auc(fpr,tpr)\n",
        "plt.plot(fpr, tpr, label='LSTM (area = {:.3f})'.format(auc_tf))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mvmjp0hdbUs"
      },
      "source": [
        "model.predict(X_test[0:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyrzeQJ5Lr8"
      },
      "source": [
        "## Federated Training and preprocessing centrailized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmtnpe0k7LkP"
      },
      "source": [
        "### Generating a decentralized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-mdaAoCIZd9"
      },
      "source": [
        "T_CLIENTS = 500\n",
        "CLIENT_SAMPLE_SIZE = len(X_train)/T_CLIENTS\n",
        "step = len(X_train)/T_CLIENTS\n",
        "\n",
        "data_fed = [ tf.data.Dataset.from_tensor_slices(\n",
        "         {\n",
        "          \"value\":X_train[int(i*step):int((i+1)*step)],\n",
        "          \"label\":Y_train[int(i*step):int((i+1)*step)]\n",
        "        }\n",
        "     ) for i in range(T_CLIENTS)]\n",
        "\n",
        "# client dataset can be accesed as data_fed[ CLIENT_ID ]\n",
        "example_dataset = data_fed[0]\n",
        "\n",
        "#   def preprocess(data_fed):\n",
        "#   def batch_format_fn(ele):\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "\n",
        "print(example_element[\"value\"].numpy()) # example_element[0] refers to X val\n",
        "print(example_element[\"label\"].numpy()) # example_element[1] refers to X val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Wpqli6Vh2a"
      },
      "source": [
        "# preprocess\n",
        "\n",
        "NUM_CLIENTS = T_CLIENTS\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = int(CLIENT_SAMPLE_SIZE//2)\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "  \n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=element[\"value\"],\n",
        "        y=element[\"label\"]\n",
        "    )\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMHiDzm2WtjI"
      },
      "source": [
        "\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch[\"x\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMom1cGNXsRR"
      },
      "source": [
        "def make_federated_data(client_data,client_ids):\n",
        "  return [\n",
        "      preprocess(client_data[x])\n",
        "      for x in client_ids\n",
        "  ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgHIDe4YB1w"
      },
      "source": [
        "sample_clients = [x for x in range(NUM_CLIENTS)]\n",
        "\n",
        "federated_train_data = make_federated_data(data_fed, sample_clients)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trf7VhCY9Mqe"
      },
      "source": [
        "### Creating the Iterative process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KncNTVBU5JXO"
      },
      "source": [
        "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "    keras_model,\n",
        "    input_spec=preprocessed_example_dataset.element_spec,\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "str(iterative_process.initialize.type_signature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybQ_rTkjRfm"
      },
      "source": [
        "state = iterative_process.initialize()\n",
        "tff_model = tf.keras.models.clone_model(model)\n",
        "tff_model.compile(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    optimizer=tf.keras.optimizers.SGD(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "def keras_evaluate(state, round_num):\n",
        "  state.model.assign_weights_to(tff_model)\n",
        "  loss, accuracy = tff_model.evaluate(X_test,Y_test, steps=2, verbose=0)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQuAfSa29UXh"
      },
      "source": [
        "### Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Z1XlOgjgqK"
      },
      "source": [
        "NUM_ROUNDS = 21\n",
        "CLIENT_PER_ROUND = 25\n",
        "for round_num in range(1, NUM_ROUNDS):\n",
        "  # 500 clients are selected for training in various rounds\n",
        "  # there are 10 rounds and each round takes data from 50 clients\n",
        "\n",
        "  clients = [round_num*x for x in range(1,CLIENT_PER_ROUND)]\n",
        "  federated_train_data = make_federated_data(data_fed, clients)\n",
        "  \n",
        "  print(\"ROUND \",round_num)\n",
        "  keras_evaluate(state, NUM_ROUNDS + 1) # eval before hand\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('\\tTrain: loss={}, accuracy={} '.format(round_num, metrics[\"train\"][\"loss\"],metrics[\"train\"][\"binary_accuracy\"]))\n",
        "\n",
        "keras_evaluate(state, NUM_ROUNDS + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL1DYMZCS30F"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# tensorflow LSTM \n",
        "Y_pred = model.predict(X_test).ravel()\n",
        "Y_pred.reshape(-1,1)\n",
        "fpr, tpr, threshold = roc_curve(Y_test.reshape(-1,1), Y_pred) \n",
        "auc_tf = auc(fpr,tpr)\n",
        "plt.plot(fpr, tpr, label='TF (area = {:.3f})'.format(auc_tf))\n",
        "\n",
        "\n",
        "# Federated Learning\n",
        "state.model.assign_weights_to(tff_model)\n",
        "\n",
        "Y_pred_tff = tff_model.predict(X_test).ravel()\n",
        "Y_pred_tff.reshape(-1,1)\n",
        "fpr_tff, tpr_tff, threshold_tff = roc_curve(Y_test.reshape(-1,1), Y_pred_tff) \n",
        "auc_tff = auc(fpr_tff,tpr_tff)\n",
        "plt.plot(fpr_tff, tpr_tff,'k', label='FL (area = {:.3f})'.format(auc_tff))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNjNgKcUmrBf"
      },
      "source": [
        "### Plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIqNg1tD8GWo"
      },
      "source": [
        "#### ROC for various SNR  (TF vs TFF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GW37W_mmqQo"
      },
      "source": [
        "# generating test-dataset for various snr\n",
        "def generate_Data_dict(FM,snr_ratio=(0.6, 0.4),sample_size=100):\n",
        "  lsnr_range=(-20,-4)\n",
        "  hsnr_range=(-4,6)\n",
        "  look_back = 2\n",
        "  N = len(FM)\n",
        "  \n",
        "  no_of_sample_hsnr = int((N*snr_ratio[0])//((hsnr_range[1] - hsnr_range[0])/2))\n",
        "  no_of_sample_lsnr = int((N*snr_ratio[1])//((lsnr_range[1] - lsnr_range[0])/2))\n",
        "  \n",
        "  X_FM = {**final_dataset(FM, range(lsnr_range[0],lsnr_range[1], 2),no_of_sample_lsnr, sample_size),\n",
        "          **final_dataset(FM, range(hsnr_range[0],hsnr_range[1], 2), no_of_sample_hsnr, sample_size)}\n",
        "\n",
        "\n",
        "  # lookback\n",
        "\n",
        "  X_FM_lb = dataset_look_back(X_FM, range(-20, 6, 2), look_back)\n",
        "\n",
        "  # final X_train and y\n",
        "\n",
        "  data = {}\n",
        "\n",
        "  for snr in range(-20, 6, 2):\n",
        "    X = X_FM_lb[snr]\n",
        "    y_ones = np.ones((X.shape[0], 1))\n",
        "\n",
        "    X_noise = create_noise_sequence(len(X)*sample_size, sample_size)\n",
        "    X_noise_lb = create_look_back(X_noise, look_back)\n",
        "    X = np.concatenate((X, X_noise_lb), axis=0)\n",
        "    Y = np.concatenate((y_ones, np.zeros((X_noise_lb.shape[0], 1))))\n",
        "\n",
        "    # reshape\n",
        "    X = np.reshape(X, (-1, 2, 1))\n",
        "    Y= np.reshape(Y,(-1,1,1))\n",
        "\n",
        "    index = [i for i in range(len(X))]\n",
        "    np.random.shuffle(index)\n",
        "    X_train = np.array([X[i] for i in index])\n",
        "    Y_train = np.array([Y[i] for i in index])\n",
        "\n",
        "    data[snr] = {\n",
        "        \"x\":X,\n",
        "        \"y\":Y\n",
        "    }\n",
        "  \n",
        "  return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DotG2romwsi_"
      },
      "source": [
        "data_snr = generate_Data_dict(FM[int(-N*(0.01)):])\n",
        "data_snr[-20][\"x\"].shape,data_snr[-20][\"y\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnmM1s8Wu7pd"
      },
      "source": [
        "# roc for various snr \n",
        "\n",
        "for snr_db in range(-20,6,2):\n",
        "  # tensorflow_lstm\n",
        "  Y_pred_tf = model.predict(data_snr[snr_db][\"x\"])\n",
        "  Y_pred_tf.reshape(-1,1)\n",
        "  fpr, tpr, threshold = roc_curve(data_snr[snr_db][\"y\"].reshape(-1,1), Y_pred_tf) \n",
        "  auc_tf = auc(fpr,tpr)\n",
        "  plt.plot(fpr, tpr, label='TF (snr = {:.3f})'.format(snr_db))\n",
        "  # tff_lstm\n",
        "  # Y_pred_tff = tff_model.predict(data_dict[snr_db][\"x\"]).ravel()\n",
        "  # Y_pred_tff.reshape(-1,1)\n",
        "  # fpr_tff, tpr_tff, threshold_tff = roc_curve(data_dict[snr_db][\"y\"].reshape(-1,1), Y_pred_tff) \n",
        "  # auc_tff = auc(fpr_tff,tpr_tff)\n",
        "  # plt.plot(fpr_tff, tpr_tff,'k', label='FL (snr = {:.3f})'.format(snr_db))\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SyUAWEdlS59"
      },
      "source": [
        "# roc for various snr \n",
        "\n",
        "for snr_db in range(-20,6,2):\n",
        "  # tensorflow_lstm\n",
        "  # Y_pred_tf = model.predict(data_snr[snr_db][\"x\"])\n",
        "  # Y_pred_tf.reshape(-1,1)\n",
        "  # fpr, tpr, threshold = roc_curve(data_snr[snr_db][\"y\"].reshape(-1,1), Y_pred_tf) \n",
        "  # auc_tf = auc(fpr,tpr)\n",
        "  # plt.plot(fpr, tpr, label='TF (snr = {:.3f})'.format(snr_db))\n",
        "  # tff_lstm\n",
        "  Y_pred_tff = tff_model.predict(data_snr[snr_db][\"x\"]).ravel()\n",
        "  Y_pred_tff.reshape(-1,1)\n",
        "  fpr_tff, tpr_tff, threshold_tff = roc_curve(data_snr[snr_db][\"y\"].reshape(-1,1), Y_pred_tff) \n",
        "  auc_tff = auc(fpr_tff,tpr_tff)\n",
        "  plt.plot(fpr_tff, tpr_tff,'k', label='FL (snr = {:.3f})'.format(snr_db))\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7s7q6AXf6YK"
      },
      "source": [
        "# Cooperative Spectrum Sensing using LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKi_e6eIEWt-"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from typing import List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik0_D1rVgE0a"
      },
      "source": [
        "# qfuncinv similar to one in matlab\n",
        "def qfuncinv(x):\n",
        "    return -1*norm.ppf(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRwY42TKgJCR"
      },
      "source": [
        "# initializing variable\n",
        "\n",
        "L = 100      # sample size\n",
        "N = 100     # number of simulations\n",
        "nodes = 5     # number of Sec Users\n",
        "\n",
        "SNR_db =  np.arange(-20,-18)    # snr range in dB\n",
        "# SNR_lin = 10**(SNR_db/10)     # snr - linear\n",
        "\n",
        "# Remove this part to include the ML model \n",
        "\n",
        "Pf = 0.2     # const probability of false alarm\n",
        "\n",
        "Threshold = (qfuncinv(Pf)/math.sqrt(L)) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt_yMKYwhMbO"
      },
      "source": [
        "#  Simulation for Pd\n",
        "\n",
        "# prediction based on lstm model\n",
        "def detection_tf(energy_stat,energy_stat_pre) -> bool:\n",
        "  return model.predict([[[energy_stat],[energy_stat_pre]]]).reshape((-1))[0] >= 0.5\n",
        "\n",
        "def detection_tff(energy_stat,energy_stat_pre) -> bool:\n",
        "  return tff_model.predict([[[energy_stat],[energy_stat_pre]]]).reshape((-1))[0] >= 0.5\n",
        "# OR_based fusion \n",
        "def detect_func_OR(arr):\n",
        "  return np.sum(arr) > 0\n",
        "\n",
        "# AND_based fusion \n",
        "def detect_func_AND(arr:List[bool])-> np.ndarray:\n",
        "  return np.multiply.reduce(arr) == 1\n",
        "\n",
        "# Majority_based fusion\n",
        "def detect_func_M(arr:List[bool])-> np.ndarray:\n",
        "  return np.count_nonzero(arr)/len(arr) >= 0.5\n",
        "\n",
        "\n",
        "Pd_OR_tf = np.array([])\n",
        "Pd_AND_tf = np.array([])\n",
        "Pd_M_tf = np.array([])\n",
        "\n",
        "Pd_OR_tff = np.array([])\n",
        "Pd_AND_tff = np.array([])\n",
        "Pd_M_tff = np.array([])\n",
        "\n",
        "SNR_db = [ i for i in range(-20,6,2)]\n",
        "for snr_db in range(-20,6,2):\n",
        "\n",
        "  detect_OR_tf = 0\n",
        "  detect_AND_tf = 0\n",
        "  detect_M_tf = 0\n",
        "\n",
        "  detect_OR_tff = 0\n",
        "  detect_AND_tff = 0\n",
        "  detect_M_tff = 0\n",
        "\n",
        "\n",
        "  print(\"At snr :\",snr_db)\n",
        "\n",
        "  Energy_stat_pre = np.array([])\n",
        "  for _ in range(N):\n",
        "    noise = np.random.randn(L,nodes) #nodesxL\n",
        "    signal = math.sqrt(10**(snr_db/10))*np.random.randn(L,nodes)\n",
        "\n",
        "    Rec_signal = signal + noise\n",
        "    #print(noise,signal,Rec_signal)\n",
        "        \n",
        "    Energy_stat = np.sum(np.abs(Rec_signal**2) ,axis=0) \n",
        "           \n",
        "    if not Energy_stat_pre.any():\n",
        "      Energy_stat_pre = Energy_stat\n",
        "\n",
        "    #print(Energy_stat[1],Energy_stat_pre[1])\n",
        "    det_tf = [detection_tf(Energy_stat[x],Energy_stat_pre[x]) for x in range(nodes)]\n",
        "    det_tff = [detection_tff(Energy_stat[x],Energy_stat_pre[x]) for x in range(nodes)]\n",
        "    \n",
        "    Energy_stat_pre = Energy_stat\n",
        "    #print(detect)\n",
        "\n",
        "    if detect_func_AND(det_tf):\n",
        "      detect_AND_tf += 1\n",
        "\n",
        "    if detect_func_AND(det_tff):\n",
        "      detect_AND_tff += 1\n",
        "\n",
        "    if detect_func_OR(det_tf):\n",
        "      detect_OR_tf += 1\n",
        "\n",
        "    if detect_func_OR(det_tff):\n",
        "      detect_OR_tff += 1\n",
        "\n",
        "    if detect_func_M(det_tf):\n",
        "      detect_M_tf += 1\n",
        "\n",
        "    if detect_func_M(det_tff):\n",
        "      detect_M_tff += 1\n",
        "\n",
        "  Pd_OR_tf = np.append(Pd_OR_tf,detect_OR_tf/N)\n",
        "  Pd_AND_tf = np.append(Pd_AND_tf,detect_AND_tf/N)\n",
        "  Pd_M_tf = np.append(Pd_M_tf,detect_M_tf/N)\n",
        "\n",
        "  Pd_OR_tff = np.append(Pd_OR_tff,detect_OR_tff/N)\n",
        "  Pd_AND_tff = np.append(Pd_AND_tff,detect_AND_tff/N)\n",
        "  Pd_M_tff = np.append(Pd_M_tff,detect_M_tff/N)\n",
        "\n",
        "#plot\n",
        "plt.plot(SNR_db,Pd_OR_tf,'bo-',SNR_db,Pd_AND_tf,'ro-',SNR_db,Pd_M_tf,'ko-',SNR_db,Pd_OR_tff,'bx--',SNR_db,Pd_AND_tff,'rx--',SNR_db,Pd_M_tff,'kx--')\n",
        "# plot properties\n",
        "plt.xlabel(\"Signal To Noise Ratio (dB)\")\n",
        "plt.ylabel(\"Probability of Detection (P_d)\")\n",
        "plt.title(\"P_d v/s SNR (TF vs TFF comparision)\")\n",
        "\n",
        "plt.legend([\"Pd_OR_tf\",\"Pd_AND_tf\",\"Pd_M_tf\",\"Pd_OR_tff\",\"Pd_AND_tff\",\"Pd_M_tff\"])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuFjLlSYqEq8"
      },
      "source": [
        "model.predict([[[100.2335050401152],[155.1100918561721]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_3iQKqMv79"
      },
      "source": [
        "N = 100\n",
        "SNR_db = np.arange(-20,5,2)\n",
        "markers = [\".\",\"o\",\"8\",\"^\"]\n",
        "i = 0\n",
        "for k in [1,2,5,10]: # number of nodes / SU\n",
        "  print(\"K =\",str(k))\n",
        "\n",
        "  Pd_OR = np.array([])\n",
        "\n",
        "  for snr_db in SNR_db:\n",
        "\n",
        "    detect_OR = 0\n",
        "\n",
        "    print(\"\\tAt snr :\",snr_db)\n",
        "\n",
        "    Energy_stat_pre = np.array([])\n",
        "    for _ in range(N):\n",
        "      noise = np.random.randn(L,k)\n",
        "\n",
        "      signal = math.sqrt(10**(snr_db/10))*np.random.randn(L,k)\n",
        "\n",
        "      Rec_signal = signal + noise \n",
        "\n",
        "      Energy_stat = np.sum(np.abs(Rec_signal**2) ,axis=0) \n",
        "\n",
        "      if not Energy_stat_pre.any():\n",
        "        Energy_stat_pre = Energy_stat\n",
        "\n",
        "      detect = [detection(Energy_stat[x],Energy_stat_pre[x]) for x in range(k)]\n",
        "\n",
        "      Energy_stat_pre = Energy_stat\n",
        "      #print(detect)\n",
        "\n",
        "      if detect_func_OR(detect):\n",
        "        detect_OR += 1\n",
        "\n",
        "\n",
        "    Pd_OR = np.append(Pd_OR,detect_OR/N)\n",
        "\n",
        "  #plot\n",
        "  plt.plot(\n",
        "      SNR_db,\n",
        "      Pd_OR,\n",
        "      marker=markers[i],\n",
        "      linestyle='dashed',\n",
        "      label=str(\"K = \"+str(k))\n",
        "      )\n",
        "  i += 1\n",
        "# plot properties\n",
        "plt.title(\"P_d v/s SNR for OR-based fusion\")\n",
        "plt.ylabel(\"Probability of Detection (P_d)\")\n",
        "plt.xlabel(\"Signal To Noise Ratio (dB)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66cbcqSJKTe"
      },
      "source": [
        "# Simulation Pf\n",
        "Pf_OR = np.array([])\n",
        "Pf_AND = np.array([])\n",
        "Pf_M = np.array([])\n",
        "\n",
        "for snr_db in SNR_db:\n",
        "\n",
        "  detect_OR = 0\n",
        "  detect_AND = 0\n",
        "  detect_M = 0\n",
        "\n",
        "  print(\"At snr :\",snr_db)\n",
        "\n",
        "  Energy_stat_pre = np.array([])\n",
        "  for _ in range(N):\n",
        "    noise = np.random.randn(L,nodes)\n",
        "\n",
        "    #signal = math.sqrt(10**(snr_db/10))*np.random.randn(L,nodes)\n",
        "\n",
        "    Rec_signal = noise  # no signal\n",
        "        \n",
        "    Energy_stat = np.sum(np.abs(Rec_signal**2) ,axis=0) \n",
        "            \n",
        "    if not Energy_stat_pre.any():\n",
        "      Energy_stat_pre = Energy_stat\n",
        "\n",
        "    detect = [detection(Energy_stat[x],Energy_stat_pre[x]) for x in range(nodes)]\n",
        "    \n",
        "    Energy_stat_pre = Energy_stat\n",
        "    #print(detect)\n",
        "\n",
        "    if detect_func_AND(detect):\n",
        "      detect_AND += 1\n",
        "        \n",
        "    if detect_func_OR(detect):\n",
        "      detect_OR += 1\n",
        "    \n",
        "    if detect_func_M(detect):\n",
        "      detect_M += 1\n",
        "      \n",
        "  Pf_OR = np.append(Pf_OR,detect_OR/N)\n",
        "  Pf_AND = np.append(Pf_AND,detect_AND/N)\n",
        "  Pf_M = np.append(Pf_M,detect_M/N)\n",
        "\n",
        "#plot\n",
        "plt.plot(SNR_db,Pf_OR,'bo--',SNR_db,Pf_AND,'rx--',SNR_db,Pf_M,'kx--')\n",
        "# plot properties\n",
        "plt.xlabel(\"Signal To Noise Ratio (dB)\")\n",
        "plt.ylabel(\"Probability of False Alarm (P_f)\")\n",
        "plt.title(\"P_f v/s SNR\")\n",
        "plt.legend([\"Pf_OR\",\"Pf_AND\",\"Pf_M\"])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}