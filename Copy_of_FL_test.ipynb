{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FL_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitvaghela/DSA_FL/blob/main/Copy_of_FL_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svCGSaj-nkhH"
      },
      "source": [
        "# LSTM time series prediction using Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXCLSy4Gnwlb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH133PxAmIwg",
        "outputId": "9ecba9cf-a540-43c1-daf3-91e15ad6b42e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 532kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 10.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 43.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 394.9MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 10.6MB 40.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 39.6MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.5.0.dev20201106 has requirement absl-py~=0.10, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.5.0.dev20201106 has requirement grpcio~=1.32.0, but you'll have grpcio 1.29.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-nightly 2.5.0.dev20201106 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFY4S2_Snf7n"
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMWb0_6TiCEY",
        "outputId": "e9fc1ce2-19ce-407c-84db-81cb39b3040f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow_federated as tff\n",
        "\n",
        "# Test the TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sai_FSFon2bi"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01sVWKEeGD2h",
        "outputId": "f96390f0-0d3b-4954-a8d6-4693fb563491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#FM = np.fromfile('96_7_20db',dtype=np.float32)\n",
        "N = 10**6\n",
        "FM = np.random.randn(N) # normal distributed channel\n",
        "FM.reshape(-1,1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.67305145],\n",
              "       [ 0.24984414],\n",
              "       [-1.67010768],\n",
              "       ...,\n",
              "       [-1.12554283],\n",
              "       [ 0.95170926],\n",
              "       [-1.15237806]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAM2qAD-oW94"
      },
      "source": [
        "### Bandpower Equation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYO80pFIoeQA"
      },
      "source": [
        "def bandpower(signal:np.array)->np.float:\n",
        "  return np.mean(signal ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNXbJyA7oi1m",
        "outputId": "f7cc58f8-09af-4307-9253-3f1afba2bc92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bandpower(FM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9977254833893389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcmk3Fto4Rv"
      },
      "source": [
        "### awgn Function\n",
        "\n",
        "Function to add noise to the signal resulting in given S/N ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWbyqmZGo8_W"
      },
      "source": [
        "def awgn(signal:np.ndarray, desired_snr:int):\n",
        "  \"\"\"Add AWGN noise to generate signal with given SNR. \n",
        "  \"\"\"\n",
        "  # Converting the SNR from dB scale to linear scale\n",
        "  snr_linear = math.pow(10, desired_snr / 10)\n",
        "  \n",
        "  # Standard normally distributed noise\n",
        "  noise = np.random.randn(signal.shape[0], 1)\n",
        "  \n",
        "  # Using the boxed formula\n",
        "  var_signal = bandpower(noise) * snr_linear\n",
        "  \n",
        "  # Normalizing the signal to have the given variance\n",
        "  normalized_signal = math.sqrt(var_signal) * (signal / math.sqrt(bandpower(signal)))\n",
        "  \n",
        "  #print(\"SNR = \" + str(10 * math.log10(bandpower(normalized_signal) / bandpower(noise))))\n",
        "  \n",
        "  return normalized_signal + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4pR1LeimNOH"
      },
      "source": [
        "## Filtering Data\n",
        "\n",
        "filtering data points to be in range $10^{-7}< signal< 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIk4gvwImMLw",
        "outputId": "aa878ca4-46e9-4e5c-914e-0011cf924d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "FM = FM[np.logical_and(FM > math.pow(10, -7), FM < 1)]\n",
        "FM = FM.reshape(FM.shape[0], 1)\n",
        "print(\"Size of FM: \" + str(FM.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of FM: (341466, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbLdLGRQmiRO"
      },
      "source": [
        "## Creating Dataset\n",
        "\n",
        "- take datapoints of size, _samples*sample_size_\n",
        "\n",
        "- add noise with desire snr\n",
        "\n",
        "- sample = $[s_1,s_2,\\cdots,s_N]$\n",
        "\n",
        "- Energy detection = $\\sum_{i=1}^{N}s_i^2$\n",
        "\n",
        "- $X[j]$ = $\\sum_{i=j*N+1}^{(j+1)*N}s_i^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LYkOJNmk0T"
      },
      "source": [
        "def create_dataset(signal, desired_snr, samples, sample_size):\n",
        "  \n",
        "  # Creating the signal with desired SNR\n",
        "  snr_signal = awgn(signal[0:samples * sample_size], desired_snr)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = snr_signal[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WulnZ62AqrIo",
        "outputId": "4e2e19c6-e88e-4193-b1ab-c10084f1e564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# sample dataset for SNR=4\n",
        "\n",
        "print(create_dataset(FM[50000:], 4, 15000, 100).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 1)\n",
            "CPU times: user 206 ms, sys: 1.88 ms, total: 208 ms\n",
            "Wall time: 209 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_gucdavtx1G"
      },
      "source": [
        "def final_dataset(signal, snr_range, samples_per_snr, sample_size):\n",
        "  X = {}\n",
        "  \n",
        "  for snr in snr_range:\n",
        "    # Creating dataset for the given SNR\n",
        "    X_snr = create_dataset(signal, snr, samples_per_snr, sample_size)\n",
        "    \n",
        "    # Indexing within the final dataset matrix X\n",
        "    X[snr] = X_snr\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4xprvuz63F"
      },
      "source": [
        "## Generating White Noise Sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxedtzrhz1Zg"
      },
      "source": [
        "def create_noise_sequence(samples, sample_size):\n",
        "  \n",
        "  # Creating white noise sequence of variance 1\n",
        "  noise = np.random.randn(samples * sample_size, 1)\n",
        "  \n",
        "  # Allocating zeros to the dataset\n",
        "  X = np.zeros((samples, 1))\n",
        "  \n",
        "  for i in range(0, samples):\n",
        "    \n",
        "    # Extracting the sample based on sample size\n",
        "    sampled_signal = noise[i * sample_size : (i + 1) * sample_size]\n",
        "    \n",
        "    # Sorting the sampled signal\n",
        "    sampled_signal = np.sort(sampled_signal, axis=0)\n",
        "    \n",
        "    # Energy detection\n",
        "    E = np.sum(sampled_signal ** 2)\n",
        "    \n",
        "    # Assigning values to the dataset\n",
        "    X[i][0] = E\n",
        "  \n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJyk2JF-75mK"
      },
      "source": [
        "## DataSet LookBack for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvufTiZ474jN"
      },
      "source": [
        "# Function for Chaning the dataset for look back  #linear dataset initially\n",
        "def create_look_back(X, look_back=1):\n",
        "  \n",
        "  # Look back dataset is initialized to be empty\n",
        "  look_back_X = []\n",
        "  \n",
        "  for i in range(len(X) - look_back + 1):\n",
        "    # Extracting an example from the dataset\n",
        "    a = X[i:(i + look_back), :]\n",
        "    \n",
        "    a = a.flatten() # (For flattening) #1D list\n",
        "    \n",
        "    # Appending to the dataset\n",
        "    look_back_X.append(a)\n",
        "  \n",
        "#  look_back_Y = []\n",
        "    \n",
        "  # Returning in numpy's array format\n",
        "  return np.array(look_back_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GMlbPO8AVW"
      },
      "source": [
        "def dataset_look_back(X_tech, snr_range, look_back):\n",
        "  X_tech_lb = {}\n",
        "  \n",
        "  # Look backs for all SNRs\n",
        "  for snr in snr_range:\n",
        "    X_tech_lb[snr] = create_look_back(X_tech[snr], look_back)\n",
        "  \n",
        "  return X_tech_lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scC_nkQK5nTW"
      },
      "source": [
        "def generate_Dataset(FM,snr_ratio=(0.8, 0.2),sample_size=100,total_sample=100000,lsnr_range=(-20,-4),hsnr_range=(-4,6),look_back = 2,eval=False):\n",
        "\n",
        "  no_of_sample_hsnr = int((total_sample*snr_ratio[0])//((hsnr_range[1] - hsnr_range[0])/2))\n",
        "  no_of_sample_lsnr = int((total_sample*snr_ratio[1])//((lsnr_range[1] - lsnr_range[0])/2))\n",
        "  \n",
        "  X_FM = {**final_dataset(FM[100000:], range(lsnr_range[0],lsnr_range[1], 2),no_of_sample_lsnr, sample_size),\n",
        "          **final_dataset(FM[100000:], range(hsnr_range[0],hsnr_range[1], 2), no_of_sample_hsnr, sample_size)}\n",
        "\n",
        "  X_noise = create_noise_sequence(100000, 100)\n",
        "\n",
        "  # lookback\n",
        "\n",
        "  X_FM_lb = dataset_look_back(X_FM, range(-20, 6, 2), look_back)\n",
        "  X_noise_lb = create_look_back(X_noise, look_back)\n",
        "\n",
        "  # final X_train and y\n",
        "\n",
        "  X = X_FM_lb[-20]\n",
        "  y = []\n",
        "\n",
        "  for snr in range(-18, 6, 2):\n",
        "    X = np.concatenate((X, X_FM_lb[snr]), axis=0)\n",
        "\n",
        "  y = np.ones((X.shape[0], 1))\n",
        " \n",
        "  # print(X.shape)\n",
        "  # print(X_noise_lb.shape)\n",
        "  X = np.concatenate((X, X_noise_lb), axis=0)\n",
        "  y_train = np.concatenate((y, np.zeros((X_noise_lb.shape[0], 1))))\n",
        "\n",
        "  # reshape\n",
        "  X_train = np.reshape(X, (-1, 2, 1))\n",
        "  Y_train = np.reshape(y_train,(-1,1,1))\n",
        "  return X_train,Y_train\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJU9EU3wBwNe"
      },
      "source": [
        "# created dataset\n",
        "X,Y = generate_Dataset(FM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGzoklAWn1D2",
        "outputId": "75d75e1b-ccbf-449f-9819-e7ea9def3eea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Shape of X,Y :\",X.shape,Y.shape) # similar to the shape we have\n",
        "\n",
        "# train,test split\n",
        "train_size = int(len(X)*0.80)\n",
        "test_size = len(X) - train_size\n",
        "\n",
        "X_train, X_test = X[:train_size,:,:],X[train_size:,:,:]\n",
        "Y_train, Y_test = Y[:train_size,:,:],Y[train_size:,:,:]\n",
        "\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(Y_train.shape,Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X,Y : (199986, 2, 1) (199986, 1, 1)\n",
            "(159988, 2, 1) (39998, 2, 1)\n",
            "(159988, 1, 1) (39998, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toI_ufHp468o",
        "outputId": "61b16388-16bc-47fd-816b-0df77c2bd2f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train[0],Y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[125.50295881],\n",
              "        [ 90.16296745]]), array([[1.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgDaE52i1rW1"
      },
      "source": [
        "## LSTM model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFVvvCA1oyA",
        "outputId": "3ec90802-770d-404a-c53e-25619bfdad41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 4)                 96        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 101\n",
            "Trainable params: 101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcsCPMCG2TiW",
        "outputId": "7879277b-a921-41c8-db81-30d06aac6aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(X_train,Y_train,validation_split=0.25,epochs=10,verbose=1)\n",
        "\n",
        "model.evaluate(X_test,Y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3750/3750 [==============================] - 11s 2ms/step - loss: 0.1304 - accuracy: 0.8213 - val_loss: 0.2549 - val_accuracy: 0.4950\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0799 - accuracy: 0.8705 - val_loss: 0.2163 - val_accuracy: 0.6687\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0761 - accuracy: 0.8748 - val_loss: 0.1795 - val_accuracy: 0.7747\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0757 - accuracy: 0.8757 - val_loss: 0.2273 - val_accuracy: 0.6356\n",
            "Epoch 5/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0748 - accuracy: 0.8769 - val_loss: 0.1776 - val_accuracy: 0.7856\n",
            "Epoch 6/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0752 - accuracy: 0.8757 - val_loss: 0.2497 - val_accuracy: 0.5786\n",
            "Epoch 7/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0756 - accuracy: 0.8751 - val_loss: 0.1531 - val_accuracy: 0.8523\n",
            "Epoch 8/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0743 - accuracy: 0.8773 - val_loss: 0.2377 - val_accuracy: 0.6208\n",
            "Epoch 9/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0746 - accuracy: 0.8771 - val_loss: 0.1843 - val_accuracy: 0.7781\n",
            "Epoch 10/10\n",
            "3750/3750 [==============================] - 9s 2ms/step - loss: 0.0743 - accuracy: 0.8775 - val_loss: 0.2294 - val_accuracy: 0.6503\n",
            "1250/1250 [==============================] - 2s 1ms/step - loss: 0.2293 - accuracy: 0.6486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22932171821594238, 0.6486324071884155]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRyulTQ4xlR",
        "outputId": "fc2986b0-f739-4d37-d99e-766b1c3dd6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.predict([\n",
        "                [[0.88069264],[0.60346049]]\n",
        "               ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999708]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyrzeQJ5Lr8"
      },
      "source": [
        "## Federated Training and preprocessing centrailized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmtnpe0k7LkP"
      },
      "source": [
        "### Generating a decentralized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-mdaAoCIZd9",
        "outputId": "86336452-8b00-4300-8d8b-f78c20e383cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "T_CLIENTS = 500\n",
        "CLIENT_SAMPLE_SIZE = 2000\n",
        "step = len(X)/T_CLIENTS\n",
        "\n",
        "data_fed = [ tf.data.Dataset.from_tensor_slices(\n",
        "     \n",
        "         {\"value\":X[int(i*step):int((i+1)*step)],\"label\":Y[int(i*step):int((i+1)*step)]}\n",
        "      \n",
        "     \n",
        "     ) for i in range(T_CLIENTS)]\n",
        "\n",
        "# client dataset can be accesed as data_fed[ CLIENT_ID ]\n",
        "example_dataset = data_fed[0]\n",
        "\n",
        "# def preprocess(data_fed):\n",
        "#   def batch_format_fn(ele):\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "\n",
        "print(example_element[\"value\"].numpy()) # example_element[0] refers to X val\n",
        "print(example_element[\"label\"].numpy()) # example_element[1] refers to X val\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[125.50295881]\n",
            " [ 90.16296745]]\n",
            "[[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Wpqli6Vh2a"
      },
      "source": [
        "# preprocess\n",
        "\n",
        "NUM_CLIENTS = 200\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 1000\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "  \n",
        "  def batch_format_fn(element):\n",
        "    return collections.OrderedDict(\n",
        "        x=element[\"value\"],\n",
        "        y=element[\"label\"]\n",
        "    )\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMHiDzm2WtjI",
        "outputId": "3e536ffa-231e-41c3-ce1b-fc5052d3d146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch[\"x\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMom1cGNXsRR"
      },
      "source": [
        "def make_federated_data(client_data,client_ids):\n",
        "  return [\n",
        "      preprocess(client_data[x])\n",
        "      for x in client_ids\n",
        "  ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgHIDe4YB1w",
        "outputId": "a6efbe6a-b216-4afc-9a8d-9e9f45d39452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_clients = [x for x in range(NUM_CLIENTS)]\n",
        "\n",
        "federated_train_data = make_federated_data(data_fed, sample_clients)\n",
        "\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of client datasets: 200\n",
            "First dataset: <PrefetchDataset shapes: OrderedDict([(x, (None, 2, 1)), (y, (None, 1, 1))]), types: OrderedDict([(x, tf.float64), (y, tf.float64)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trf7VhCY9Mqe"
      },
      "source": [
        "### Creating the Iterative process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KncNTVBU5JXO",
        "outputId": "1b639622-c26a-4434-9b50-510be4f8e956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.LSTM(4,input_dim=1),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "    keras_model,\n",
        "    input_spec=preprocessed_example_dataset.element_spec,\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.BinaryCrossentropy()]\n",
        "  )\n",
        "\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "str(iterative_process.initialize.type_signature)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'( -> <model=<trainable=<float32[1,16],float32[4,16],float32[16],float32[4,1],float32[1]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zybQ_rTkjRfm"
      },
      "source": [
        "state = iterative_process.initialize()\n",
        "\n",
        "def keras_evaluate(state, round_num):\n",
        "  state.model.assign_weights_to(model)\n",
        "  loss, accuracy = model.evaluate(example_dataset, steps=2, verbose=0)\n",
        "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQuAfSa29UXh"
      },
      "source": [
        "### Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7Z1XlOgjgqK",
        "outputId": "142b880c-72db-41af-b75a-abad64071ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_ROUNDS = 5\n",
        "for round_num in range(1, NUM_ROUNDS):\n",
        "  # 200 clients of 500 clients are selected for training in various rounds\n",
        "  # there are 20 rounds and each round takes data from 10 clients\n",
        "  clients = [round_num*x for x in range(1,11)]\n",
        "  federated_train_data = make_federated_data(data_fed, clients)\n",
        "  state, metrics = iterative_process.next(state, federated_train_data)\n",
        "  print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "keras_evaluate(state, NUM_ROUNDS + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.5536838), ('loss', 0.18079454)]))])\n",
            "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.55420613), ('loss', 0.18109746)]))])\n",
            "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.5518121), ('loss', 0.17995524)]))])\n",
            "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('binary_crossentropy', 0.54022455), ('loss', 0.1742653)]))])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-aa0a7201ab6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'round {:2d}, metrics={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mkeras_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_ROUNDS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-3416e013ea27>\u001b[0m in \u001b[0;36mkeras_evaluate\u001b[0;34m(state, round_num)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkeras_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_weights_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tEval: loss={l:.3f}, accuracy={a:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m       (graph_function,\n\u001b[0;32m-> 2969\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2970\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2971\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3384\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3385\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3386\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3388\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3308\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3309\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3232\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3233\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3234\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3236\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1236 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1227 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2731 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3420 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1220 run_step  **\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1186 test_step\n        y_pred = self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1000 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1, 1) dtype=float64>, <tf.Tensor 'IteratorGetNext:1' shape=(2, 1) dtype=float64>]\n"
          ]
        }
      ]
    }
  ]
}